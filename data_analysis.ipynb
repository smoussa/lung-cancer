{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dicom\n",
    "import os\n",
    "import scipy.ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from skimage import measure, morphology\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import itertools\n",
    "from IPython.display import FileLink\n",
    "\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGES_DIR = 'data/sample_images/'\n",
    "# patient_ids = os.listdir(IMAGES_DIR)\n",
    "# patient_ids = [f for f in patient_ids if len(f) == 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_dir = 'data/images/stage1/'\n",
    "labels = pd.read_csv('data/stage1_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_scan(patient_id):\n",
    "    dicom_files = glob(os.path.join(images_dir, patient_id, '*.dcm'))\n",
    "    slices = [dicom.read_file(f) for f in dicom_files]\n",
    "    slices.sort(key = lambda x: float(x.ImagePositionPatient[2]))\n",
    "    return slices\n",
    "\n",
    "def rescale(slices):\n",
    "    # normalise pixel array values (HU units?)\n",
    "    image = np.stack([s.pixel_array for s in slices])\n",
    "    image = image.astype(np.int16)\n",
    "    image[image == -2000] = 0 # 0 + intercept = 0 + -1024 = -1024 (air)\n",
    "    for slice_idx in range(len(slices)):\n",
    "        image[slice_idx] *= np.int16(slices[slice_idx].RescaleSlope) # slope\n",
    "        image[slice_idx] += np.int16(slices[slice_idx].RescaleIntercept) # intercept\n",
    "    return image\n",
    "\n",
    "def resample(image, slices):\n",
    "    # make distance between pixels 1mm\n",
    "    try:\n",
    "        slice_thickness = np.abs(slices[0].ImagePositionPatient[2] - slices[1].ImagePositionPatient[2])\n",
    "    except:\n",
    "        slice_thickness = np.abs(slices[0].SliceLocation - slices[1].SliceLocation)\n",
    "\n",
    "    for s in slices:\n",
    "        s.SliceThickness = slice_thickness\n",
    "    \n",
    "    current_shape = np.array([slices[0].SliceThickness] + slices[0].PixelSpacing, dtype=np.float32)\n",
    "    new_image = scipy.ndimage.interpolation.zoom(image, current_shape, mode='nearest', order=0)\n",
    "    \n",
    "    return new_image\n",
    "\n",
    "def normalise(image):\n",
    "    minimum = np.min(image)\n",
    "    maximum = np.max(image)\n",
    "    new_image = (image - minimum) / (maximum - minimum)\n",
    "    # ni -= 0.25 # global pixel mean\n",
    "    return new_image\n",
    "\n",
    "def preprocess(patient_id):\n",
    "    slices = load_scan(patient_id)\n",
    "    original_image = rescale(slices) # 3D image\n",
    "    new_image = resample(original_image, slices)\n",
    "    new_image = normalise(new_image)\n",
    "    new_image = np.swapaxes(new_image, 0, 2)\n",
    "    new_image = np.reshape(new_image, new_image.shape + (1,))\n",
    "    return new_image\n",
    "\n",
    "def get_patient_label(patient_id):\n",
    "    return labels[labels['id'] == patient_id]['cancer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organise data into train, valid and test dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split ids into train and valid\n",
    "valid_percent = 0.15\n",
    "pos = labels[labels['cancer'] == 1]\n",
    "neg = labels[labels['cancer'] == 0]\n",
    "pos_valid_ids = set(pos.sample(frac=valid_percent)['id'])\n",
    "neg_valid_ids = set(neg.sample(frac=valid_percent)['id'])\n",
    "pos_train_ids = set(pos['id']).difference(pos_valid_ids)\n",
    "neg_train_ids = set(neg['id']).difference(neg_valid_ids)\n",
    "# len(pos_train_ids), len(neg_train_ids), len(pos_valid_ids), len(neg_valid_ids)\n",
    "\n",
    "pos_train_ids = list(pos_train_ids)\n",
    "neg_train_ids = list(neg_train_ids)\n",
    "pos_valid_ids = list(pos_valid_ids)\n",
    "neg_valid_ids = list(neg_valid_ids)\n",
    "np.random.shuffle(pos_train_ids)\n",
    "np.random.shuffle(neg_train_ids)\n",
    "np.random.shuffle(pos_valid_ids)\n",
    "np.random.shuffle(neg_valid_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def gen_data(patient_ids):\n",
    "#     for patient_id in iter(patient_ids):\n",
    "#         yield preprocess(patient_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(patient_ids):\n",
    "    while 1:\n",
    "        for i in range(10): # 10 * 32 = 320 -> # of training samples\n",
    "            print(\"i = \" + str(i))\n",
    "            ids = patient_ids[i*32:(i+1)*32]\n",
    "            X = [preprocess(patient_id) for patient_id in ids]\n",
    "            y = [get_patient_label for patient_id in ids]\n",
    "            yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = gen_data(pos_train_ids + neg_train_ids)\n",
    "# y_train = np.vstack((np.ones(len(pos_train_ids)).reshape(-1, 1), np.zeros(len(neg_train_ids)).reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from theano.sandbox import cuda\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv3D\n",
    "from keras.layers import MaxPooling3D\n",
    "from keras.layers import AveragePooling3D\n",
    "from keras.layers import GlobalAveragePooling3D\n",
    "from keras.layers import GlobalMaxPooling3D\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.utils.layer_utils import convert_all_kernels_in_model\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.preprocessing import image as k_image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(K.image_data_format()) # channels last\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/fchollet/deep-learning-models/blob/master/inception_v3.py\n",
    "channel_axis = 4\n",
    "\n",
    "def conv_block(x, filters, h, w, z, padding='same', strides=(1, 1, 1)):\n",
    "    x = Conv3D(filters,\n",
    "               (h, w, z), # height, width, z\n",
    "               strides=strides,\n",
    "               padding=padding,\n",
    "               use_bias=False)(x)\n",
    "    x = BatchNormalization(axis=channel_axis, scale=False)(x) # batch norm axis=4 ??\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def inception_block(x):\n",
    "    \n",
    "    x = conv_block(x, 32, 3, 3, 3, strides=(2, 2, 2), padding='valid')\n",
    "    x = conv_block(x, 32, 3, 3, 3, padding='valid') # padding same or valid ??\n",
    "    x = conv_block(x, 64, 3, 3, 3)\n",
    "    x = MaxPooling3D((3, 3, 3), strides=(2, 2, 2))(x)\n",
    "\n",
    "    x = conv_block(x, 80, 1, 1, 1, padding='valid')\n",
    "    x = conv_block(x, 192, 3, 3, 3, padding='valid')\n",
    "    x = MaxPooling3D((3, 3, 3), strides=(2, 2, 2))(x)\n",
    "\n",
    "    branch1 = conv_block(x, 64, 1, 1, 1)\n",
    "\n",
    "    branch5 = conv_block(x, 48, 1, 1, 1)\n",
    "    branch5 = conv_block(branch5, 64, 5, 5, 5)\n",
    "\n",
    "    branch3 = conv_block(x, 64, 1, 1, 1)\n",
    "    branch3 = conv_block(branch3, 96, 3, 3, 3)\n",
    "    branch3 = conv_block(branch3, 96, 3, 3, 3)\n",
    "\n",
    "    branch_pool = AveragePooling3D((3, 3, 3), strides=(1, 1, 1), padding='same')(x)\n",
    "    branch_pool = conv_block(branch_pool, 32, 1, 1, 1)\n",
    "    \n",
    "    x = layers.concatenate(\n",
    "            [branch1, branch5, branch3, branch_pool],\n",
    "            axis=channel_axis)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    model_input = Input(shape=(None, None, None, 1)) # (x, y, z, channels)\n",
    "    x = inception_block(model_input)\n",
    "    x = GlobalAveragePooling3D(name='avg_pool')(x)\n",
    "    model_output = Dense(1, activation='softmax', name='predictions')(x)\n",
    "    model = Model(model_input, model_output, name='moses')\n",
    "    model.compile(optimizer='rmsprop', # change to Adam() or even Eve() ??\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary(line_length=125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.expand_dims(preprocess(pos_train_ids[0]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = np.vstack((X,X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(trn, np.array([[1], [1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.fit_generator(gen_data(pos_train_ids + neg_train_ids), steps_per_epoch=100, epochs=1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
